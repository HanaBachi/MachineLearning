{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## Subsurface Data Analytics \n",
    "\n",
    "## Advanced Clustering\n",
    "\n",
    "\n",
    "### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "### Exercise: K-means Clustering, Density- and Spectral-based Clustering for Inferential Machine Learning \n",
    "\n",
    "Here's a simple workflow, demonstration of K-means, density- and spectral-based clustering (for automated category assignment) for subsurface modeling workflows. This should help you get started with inferential methods to find patterns in your subsurface data sets.  \n",
    "\n",
    "#### Clustering Methods\n",
    "\n",
    "* **k-Means clustering** as a centroid-based clustering method with predetermined number of clusters\n",
    "\n",
    "* **DBSCAN**, density-based spatial clustering of applications with noise, as a density-based clustering method without predetermined number of clusters\n",
    "\n",
    "* **Spectral clustering** as a hierarchical connectivity-based clustering method with predetermined number of clusters\n",
    "\n",
    "#### Objective \n",
    "\n",
    "In the PGE 383: Subsurface Machine Learning class I want to provide hands-on experience with building subsurface modeling workflows. Python provides an excellent vehicle to accomplish this. I have coded a package called GeostatsPy with GSLIB: Geostatistical Library (Deutsch and Journel, 1998) functionality that provides basic building blocks for building subsurface modeling workflows. \n",
    "\n",
    "The objective is to remove the hurdles of subsurface modeling workflow construction by providing building blocks and sufficient examples. This is not a coding class per se, but we need the ability to 'script' workflows working with numerical methods.    \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/). \n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal. \n",
    "3. In the terminal type: pip install geostatspy. \n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality. \n",
    "\n",
    "#### Install Packages\n",
    "\n",
    "We will include the standard packages for DataFrames and ndarrays and add sci-kit-learn (sklearn) for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy.GSLIB as GSLIB          # GSLIB utilies, visualization and wrapper\n",
    "import geostatspy.geostats as geostats    # GSLIB methods convert to Python        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed with Anaconda 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                        # ndarrys for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import os                                 # set working directory, run executables\n",
    "import matplotlib.pyplot as plt           # for plotting\n",
    "import copy                               # for deep copies\n",
    "from sklearn.neighbors import NearestNeighbors # nearest neighbours function to calculate eps hyperparameter\n",
    "from sklearn.preprocessing import MinMaxScaler # min/max normalization\n",
    "from sklearn.cluster import KMeans        # k-means clustering\n",
    "from sklearn.cluster import DBSCAN        # DBSCAN clustering\n",
    "from sklearn.cluster import SpectralClustering # spectral clustering\n",
    "cmap = plt.cm.inferno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')         # ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Functions\n",
    "\n",
    "n/a\n",
    "\n",
    "#### Set the working directory\n",
    "\n",
    "I always like to do this so I don't lose files and to simplify subsequent read and writes (avoid including the full address each time). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"d:/PGE383\")                     # set the working directory with the input data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "Let's load the provided multivariate, spatial datasets.  It is a comma delimited file with: \n",
    "\n",
    "* you can select datasets 1-4 below.\n",
    "\n",
    "We load it with the pandas 'read_csv' function into a data frame we called 'df' and then preview it to make sure it loaded correctly.\n",
    "\n",
    "**Python Tip: using functions from a package** just type the label for the package that we declared at the beginning:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "so we can access the pandas function 'read_csv' with the command: \n",
    "\n",
    "```python\n",
    "pd.read_csv()\n",
    "```\n",
    "\n",
    "but read csv has required input parameters. The essential one is the name of the file. For our circumstance all the other default parameters are fine. If you want to see all the possible parameters for this function, just go to the docs [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).  \n",
    "\n",
    "* The docs are always helpful\n",
    "* There is often a lot of flexibility for Python functions, possible through using various inputs parameters\n",
    "\n",
    "also, the program has an output, a pandas DataFrame loaded from the data.  So we have to specficy the name / variable representing that new object.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"ML_project_HB.csv\")  \n",
    "```\n",
    "\n",
    "Let's run this command to load the data and then this command to extract a random subset of the data.\n",
    "\n",
    "```python\n",
    "df = df.sample(frac=.30, random_state = 73073); \n",
    "df = df.reset_index()\n",
    "```\n",
    "\n",
    "We do this to reduce the number of data for ease of visualization (hard to see if too many points on our plots).\n",
    "\n",
    "After the walk through, we will switch to experiential and you will load a new dataset and build your own clusters!\n",
    "\n",
    "```python\n",
    "experiential = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"https://raw.githubusercontent.com/ibrahimUT/MLProject/main/Density_Por_data.csv\")\n",
    "df_train = df.iloc[0:80,:]                                  # extract a training set, note samples are random ordered\n",
    "df_test = df.iloc[80:]                                      # extract a testing set, note samples are random ordered\n",
    "df.head()         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "col = ['DTC','Gr','CALI']\n",
    "units = ['microsec/ft','%']\n",
    "xlim = [0,1200]; ylim = [0,30]; xlabel = 'Acoustic Impedance (kg/m^2s*10^3)'; ylabel = 'Porosity (%)'\n",
    "df = pd.read_csv(\"ML_project_HB.csv\")  \n",
    "df = pd.read_csv(r'C:/Users/hanab/Downloads/nonlinear_facies_v3.csv')    # load our data table\n",
    "df_train = df.iloc[0:80,:]                                  # extract a training set, note samples are random ordered\n",
    "df_test = df.iloc[80:]                                      # extract a testing set, note samples are random ordered\n",
    "df.head()                                                   # preview the DataFrame\n",
    " \n",
    "    \n",
    "df = df[[col[0],col[1],col[2]]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the proportion of facies.\n",
    "\n",
    "* we will not use these, but they indicate the proportion of samples in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacies\u001b[39m\u001b[38;5;124m'\u001b[39m],alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m,bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m1.5\u001b[39m,\u001b[38;5;241m2.5\u001b[39m,\u001b[38;5;241m3.5\u001b[39m],label \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShale\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSandShale\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m2.5\u001b[39m],density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacies Proportions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacies\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(df['Facies'],alpha=0.2,color=\"red\",edgecolor=\"black\",bins=[0.5,1.5,2.5,3.5],label = ['Shale','SandShale','Sand'],range=[0.5,2.5],density = True)\n",
    "plt.title('Facies Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(1.0, 3.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features are quite incompatible.  They have dramatically different:\n",
    "\n",
    "* magnitudes / averages\n",
    "\n",
    "* variances / ranges\n",
    "\n",
    "We should make a normalized version of each.  We will scale the variables to range from 0 to 1.  \n",
    "\n",
    "* There is no distribution shape change.\n",
    "\n",
    "We will use these normalized values for calculating distance in our workflow:\n",
    "\n",
    "* to remove the influence of magnitude and range on our similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = MinMaxScaler(); \n",
    "features = [col[0],col[1]]\n",
    "\n",
    "df['n' + col[0]] = transform.fit_transform(df.loc[:,features].values)[:,0] # standardize the data features to mean = 0, var = 1.0\n",
    "df['n' + col[1]] = transform.fit_transform(df.loc[:,features].values)[:,1] # standardize the data features to mean = 0, var = 1.0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that our normalized porosity and acoustic impedance now range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the actual facies categories. \n",
    "\n",
    "* for this workflow this is our inaccessible truth model\n",
    "\n",
    "* we will leave these categories out and attempt to automate their assignment with a variety of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['Facies'] == 1]; df2 = df[df['Facies'] == 2]; df3 = df[df['Facies'] == 3]\n",
    "\n",
    "if dataset == 4:\n",
    "    df4 = df[df['Facies'] == 4]; df5 = df[df['Facies'] == 5]; df6 = df[df['Facies'] == 6]\n",
    "\n",
    "# plot the result\n",
    "plt.subplot(111)\n",
    "if dataset == 4:\n",
    "    plt.scatter(df1[col[0]],df1[col[1]],color = 'blue',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '1')\n",
    "    plt.scatter(df2[col[0]],df2[col[1]],color = 'red',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '2')\n",
    "    plt.scatter(df3[col[0]],df3[col[1]],color = 'green',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '3')\n",
    "    plt.scatter(df4[col[0]],df4[col[1]],color = 'orange',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '4')\n",
    "    plt.scatter(df5[col[0]],df5[col[1]],color = 'black',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '5')\n",
    "    plt.scatter(df6[col[0]],df6[col[1]],color = 'yellow',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = '6')\n",
    "\n",
    "else:\n",
    "    plt.scatter(df1[col[0]],df1[col[1]],color = 'blue',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = 'sand')\n",
    "    plt.scatter(df2[col[0]],df2[col[1]],color = 'red',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = 'shale')\n",
    "    plt.scatter(df3[col[0]],df3[col[1]],color = 'green',alpha = 0.2,cmap = cmap,vmin = 1, vmax = 4,label = 'sandshale')\n",
    "    \n",
    "plt.xlim(0,25)\n",
    "plt.ylim(0.1200)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(xlim[0], xlim[1], (xlim[1]-xlim[0])/10))\n",
    "ax.set_xticks(np.arange(xlim[0], xlim[1], (xlim[1]-xlim[0])/40), minor=True)\n",
    "ax.set_yticks(np.arange(ylim[0], ylim[1], (ylim[1]-ylim[0])/10))\n",
    "ax.set_yticks(np.arange(ylim[0], ylim[1], (ylim[1]-ylim[0])/40), minor=True)\n",
    "\n",
    "\n",
    "plt.grid(which = 'both',color = 'black',alpha = 0.2)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "plt.xlabel(xlabel); \n",
    "plt.ylabel(ylabel); plt.title('Nonlinear Multivariate Facies v1')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's ignore the truth facies cateogrical assignments and move forward.\n",
    "\n",
    "#### Some Parameters\n",
    "\n",
    "From the summary statistics we can assign a reasonable minimum and maximum for each feature.  \n",
    "\n",
    "* We will use this for plotting.\n",
    "\n",
    "We will also set the random number seed to ensure that the program does the same thing everytime it is run.\n",
    "\n",
    "* Change the seed number for a different result\n",
    "\n",
    "We will set the number of prototypes / clusters, *K*\n",
    "\n",
    "We define a dictionary with the color code for each cluster, $k = 1,\\ldots,K$.  Given 7 codes currently, there will be an error if $K$ is set larger than 7.  Add more color codes to the dictionary to allow for mor categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(210)\n",
    "K = 3                                     # number of prototypes\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b', 4: 'm', 5: 'c', 6: 'k', 7: 'w'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Training Data\n",
    "\n",
    "In this exercise, we want to use K-means clustering provide facies based on acoustic impedance and porosity predictor features. \n",
    "\n",
    "* This allows use to group rock with similar petrophysical and geophysical properties.\n",
    "\n",
    "Let's start by looking at the scatterplot of our training data features, porosity and acoustic impedance.  \n",
    "\n",
    "* We will look at the data in original units and normalized units through this entire exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot our training data  \n",
    "plt.subplot(121)\n",
    "plt.scatter(df[col[0]], df[col[1]], c=\"black\", alpha = 0.4, linewidths=1.0, edgecolors=\"black\")\n",
    "plt.title(col[1] + ' vs. ' + col[0]); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(df['n' + col[0]], df['n' + col[1]], c=\"black\", alpha = 0.4, linewidths=1.0, edgecolors=\"black\")\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0]); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' (normalized)')\n",
    "plt.xlim(0.0,1.0)\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Means Clustering\n",
    "\n",
    "First we start with k-means clustering. \n",
    "\n",
    "* I have an entire demonstration dedicated to [k-means cluster](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Clustering.ipynb).\n",
    "\n",
    "* We use k-means to baseline the subsequent approaches that we will attempt.\n",
    "\n",
    "Let's be mindful of the assumptions of k-means clustering:\n",
    "\n",
    "* spherical variability over the features\n",
    "\n",
    "* same variability over each feature (after our min/max normalization)\n",
    "\n",
    "* prior probability of membership in all clusters, equal number of samples in each\n",
    "\n",
    "This dataset violates these assumptions with non spherical shapes, inequal proportions of samples in each cluster (we know this because we have the truth facies and we can peak, or by ocular inspection).\n",
    "\n",
    "* Given the nonconvexity of the sample data in feature space this should not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(df[['n'+col[0],'n'+col[1]]].values)\n",
    "df['kMeans'] = kmeans.labels_ + 1\n",
    "\n",
    "cmap = plt.cm.inferno\n",
    "\n",
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "plt.scatter(df[col[0]], df[col[1]], c='black',alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "plt.title(col[1] + ' vs. ' + col[0] + ' with k-Means Clusters'); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(132)                          # plot the training data and K prototypes\n",
    "plt.scatter(df['n' + col[0]], df['n' + col[1]], c=df['kMeans'], alpha = 0.4, linewidths=1.0, edgecolors=\"black\", cmap = cmap)\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0] + ' with k-Means Clusters'); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' (normalized)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.2, hspace=0.2)\n",
    "\n",
    "plt.subplot(133)\n",
    "N, bins, patches  = plt.hist(df['kMeans'],alpha=0.2,edgecolor=\"black\",bins=[0.5,1.5,2.5,3.5],range=[0.5,2.5],density = True)\n",
    "patches[0].set_facecolor('black'); patches[1].set_facecolor('red'); patches[2].set_facecolor('yellow');\n",
    "plt.title('Group Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(1.0, 3.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there are issues:\n",
    "\n",
    "* the non-convex, nonlinear shapes, and low-density representation of edges of clusters are not captured well\n",
    "\n",
    "As a results obvious connected geometries are broken up.\n",
    "\n",
    "##### Clustering without Normalization / Standardization\n",
    "\n",
    "One of the critical assumptions of clustering is that the variability is the same over each feature.\n",
    "\n",
    "* an exception would be if the features have the same units and the variability differences are meaningful \n",
    "\n",
    "Let's take this dataset and draw it to scale (to show what a distance metric would see in original units.\n",
    "\n",
    "* we rotate the plot and provide an approximate visualization with porosity 1 unit equal to permeability 1 unit on the plot\n",
    "\n",
    "* effectively the dataset looks 1D to the clustering algorithm, difference in porosity becomes meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "plt.scatter(df[col[1]],df[col[0]], c='black',alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "plt.title(col[1] + ' vs. ' + col[0] + ' with k-Means Clusters'); plt.ylabel(ylabel); plt.xlabel(xlabel)\n",
    "plt.ylim(xlim)\n",
    "plt.xlim(ylim)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=8.5, top=0.10, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further motivate the use of normalization/standardization, let's apply k-means clusters on the original data (without transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_orig = KMeans(n_clusters=K, random_state=0).fit(df[[col[0],col[1]]].values)\n",
    "df['kMeans_orig'] = kmeans_orig.labels_ + 1\n",
    "\n",
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "plt.scatter(df[col[0]], df[col[1]], c=df['kMeans_orig'],alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "plt.title('Permeability vs. Porosity with k-Means Clusters'); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(132)                          # plot the training data and K prototypes\n",
    "plt.scatter(df['n'+col[0]], df['n'+col[1]], c=df['kMeans_orig'], alpha = 0.4, linewidths=1.0, edgecolors=\"black\", cmap = cmap)\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0] + ' with k-Means Clusters'); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' (normalized)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.2, hspace=0.2)\n",
    "\n",
    "plt.subplot(133)\n",
    "N, bins, patches  = plt.hist(df['kMeans_orig'],alpha=0.2,edgecolor=\"black\",bins=[0.5,1.5,2.5,3.5],range=[0.5,2.5],density = True)\n",
    "patches[0].set_facecolor('black'); patches[1].set_facecolor('red'); patches[2].set_facecolor('yellow');\n",
    "plt.title('Group Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(1.0, 3.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the difference in feature range, for non-normalized data:\n",
    "\n",
    "* the clustering is conducted over the feature with greater range and the other feature does not have a significant impact\n",
    "* see the horizontal boundaries (in permeability only) between the groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering\n",
    "\n",
    "The DBSCAN approach randomly finds clusters of samples (high density) is well suited to working with data with an arbitrary (even non-convex) shape and with noise.\n",
    "\n",
    "The two parameters include:\n",
    "\n",
    "* **eps** - maximum distance between to samples in features space for a sample to be included in the neighbourhood of the other sample. Too small a value will result in many clusters and outliers, and too large a value will result in one cluster with all the data together.\n",
    "\n",
    "* **min_samples** - minimum number of samples in a neighbourhood to spawn a new cluster. Use a larger value for noisy data to prevent identifying clusters due to a few outliers.\n",
    "\n",
    "The method proceeds through the dataset and assigns the samples as:\n",
    "\n",
    "* **core** point if there are $\\ge$ **min_samples** within **eps** distance from the sample\n",
    "\n",
    "* **border** point if there are $lt$ **min_samples** within **eps** distance from the sample, but the sample is within **eps** distance of a core point.\n",
    "\n",
    "* **outlier** point if there are $lt$ **min_samples** within **eps** distance from the sample and the sample is not within **eps** distance of a core point\n",
    "\n",
    "Once the points are assigned these labels, all connected core points and their associate border points are assigned to an unique cluster.  Outlier points are left as outliers without an assigned cluster.  To understand the cluster assigments we should explain the following forms of connection.\n",
    "\n",
    "**directly density reachable** - point X is directly density reachable from A, if A is a core point and X belongs to the neighborhood (distance $le$ eps) from A.\n",
    "\n",
    "**density-reachable** - point Y is density reachable from A if Y belongs to a neighborhood of a core point that can reached from A. This would require a chain of core points each belonging the previous core points and the last core point including point Y.\n",
    "\n",
    "**density-connected** - points A and B are density-connected if there is a point Z that is density-reachable from both points A and B.\n",
    "\n",
    "**density-based cluster** - a nonempty set where all points are density-connected to eachother. \n",
    "\n",
    "The approach iterates as follows:\n",
    "\n",
    "1. all points are labled as unvisited\n",
    "\n",
    "2. randomly visit an unvisited sample\n",
    "\n",
    "3. check if a core point ($ge$ min_sample within eps distance), if so label as core otherwise label as outlier\n",
    "\n",
    "4. now visit all points within eps distance of the core point, determine if core, otherwise label as border point\n",
    "\n",
    "5. recusive operation where all points within eps distance of new core points are checked\n",
    "\n",
    "6. once this is exhausted then randomly visit an unvisited point\n",
    "\n",
    "This apporach may be thought of as identify and grow/merge clusters guided by local point density.\n",
    "\n",
    "\n",
    "After some careful interations of these parameters we get the following result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.029, min_samples=15).fit(df[['n'+col[0],'n'+col[1]]].values)\n",
    "df['DBSCAN'] = dbscan.labels_ + 1\n",
    "\n",
    "cmap = plt.cm.inferno\n",
    "\n",
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "df_in = df.loc[(df['DBSCAN'] != 0)]\n",
    "plt.scatter(df_in[col[0]], df_in[col[1]], c=df_in['DBSCAN'], alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "df_outlier = df.loc[(df['DBSCAN'] == 0)]\n",
    "plt.scatter(df_outlier[col[0]],df_outlier[col[1]],c='black',s = 50,marker = 'x',edgecolor='k',cmap = cmap)\n",
    "plt.title(col[1] + ' vs. ' + col[0] + ' with DBSCAN'); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(132)                          # plot the training data and K prototypes\n",
    "plt.scatter(df_in['n'+col[0]], df_in['n'+col[1]], c=df_in['DBSCAN'], alpha = 0.4, linewidths=1.0, edgecolors=\"black\", cmap = cmap)\n",
    "plt.scatter(df_outlier['n'+col[0]],df_outlier['n'+col[1]],c='black',s = 50,marker = 'x',edgecolor='k',cmap = cmap)\n",
    "# option to visualize the identified core samples\n",
    "#plt.scatter(dbscan.components_[:,0],dbscan.components_[:,1],c='red',s=50,marker = 's',edgecolor='k',cmap = cmap,alpha = 0.2)\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0] + ' with k-Means Clusters'); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' (normalized)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.2, hspace=0.2)\n",
    "\n",
    "plt.subplot(133)\n",
    "N, bins, patches  = plt.hist(df['DBSCAN'],alpha=0.2,edgecolor=\"black\",bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5,6.5],range=[0.5,2.5],density = True)\n",
    "patches[1].set_facecolor('black'); patches[3].set_facecolor('magenta'); patches[4].set_facecolor('red')\n",
    "patches[5].set_facecolor('yellow'); patches[6].set_facecolor('white'); patches[2].set_facecolor('gray');\n",
    "plt.title('Group Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.0, 6.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the sensivity in the parameters, by looking at the results over a $3 \\times 3$ combinatorial of hyperparameters. Run this the first time as is and then try making changes the the values in this code.\n",
    "\n",
    "```python\n",
    "eps_mat = [0.026,0.029,0.032] \n",
    "min_sample_mat = [10, 15, 20]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_mat = [0.026,0.029,0.032] \n",
    "min_sample_mat = [10, 15, 20]\n",
    "\n",
    "index = 1\n",
    "for eps in eps_mat:\n",
    "    for min_sample in min_sample_mat:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_sample).fit(df[['n'+col[0],'n'+col[1]]].values)\n",
    "        df['DBSCAN'] = dbscan.labels_ + 1\n",
    "        \n",
    "        plt.subplot(3,3,index)                          # plot the assigned training data and K prototypes\n",
    "        df_in = df.loc[(df['DBSCAN'] != 0)]\n",
    "        plt.scatter(df_in[col[0]], df_in[col[1]], c=df_in['DBSCAN'], alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "        df_outlier = df.loc[(df['DBSCAN'] == 0)]\n",
    "        plt.scatter(df_outlier[col[0]],df_outlier[col[1]],c='black',s = 50,marker = 'x',edgecolor='k',cmap = cmap)\n",
    "        plt.title(col[1] + ' vs. ' + col[0] + ' with DBSCAN, eps = ' + str(eps) + ', min sample = ' + str(min_sample)); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        index = index + 1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=3.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use a more repeatable method to calculate the DBSCAN hyperparameters.\n",
    "\n",
    "* **min samples** - we will assume 15 \n",
    "\n",
    "* **eps** - we will use the nearest neighbour function to estimate it given the minimum number of samples\n",
    "\n",
    "Let's calculate the nearest neighbour to each sample data in normalized feature space with the command:\n",
    "\n",
    "```python \n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(df[['nPor','nPerm']].values)\n",
    "distances, indices = nbrs.kneighbors(df[['nPor','nPerm']].values)\n",
    "```\n",
    "\n",
    "We have a $n \\times k$ array with the distances and the sample data indices:\n",
    "\n",
    "* for each $n$ sample we have a row of nearest neighbour distances up to k nearest neighbours (closest to furthest in the columns), note the first neighbour is sample itself; therefore the first column is 0.\n",
    "\n",
    "* we will then sort the rows in ascending order and plot relative to the sample index, $i = 1,\\ldots,n-1$\n",
    "\n",
    "Then we will plot the distances vs. the sample index in the ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2,radius=999.0,algorithm='kd_tree')\n",
    "nbrs = neigh.fit(df[['n' + col[0],'n' + col[1]]].values)\n",
    "distances, indices = nbrs.kneighbors(df[['n' + col[0],'n' + col[1]]].values)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances, c = 'red'); plt.xlabel('Sorted Ascending, Data Index'); plt.ylabel('Intersample Distance in Feature Space')\n",
    "plt.title('Nearest Neighbour ')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.00, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for the nearest neighbour distance with the maximum positive curvature, known as the 'elbow' of the nearest neighbour plot \n",
    "\n",
    "* this provides an indication of the cluster sizes\n",
    "\n",
    "* we would estimate eps as just above 0.01 in this case.   \n",
    "\n",
    "Some observations from DBSCAN\n",
    "\n",
    "* the results are quite sensitive to the selection of **eps** and **min_samples**.\n",
    "\n",
    "* in the sparse part of the 'cluster' many of the data are assigned to other groups are remain as outliers\n",
    "\n",
    "In this case, the result with this estimated eps hyperparameter is not satisfying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.025, min_samples=15).fit(df[['n' + col[0],'n' + col[1]]].values)\n",
    "df['DBSCAN'] = dbscan.labels_ + 1\n",
    "\n",
    "cmap = plt.cm.inferno\n",
    "\n",
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "df_in = df.loc[(df['DBSCAN'] != 0)]\n",
    "plt.scatter(df_in[col[0]], df_in[col[1]], c=df_in['DBSCAN'], alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "df_outlier = df.loc[(df['DBSCAN'] == 0)]\n",
    "plt.scatter(df_outlier[col[0]],df_outlier[col[1]],c='black',s = 50,marker = 'x',edgecolor='k',cmap = cmap)\n",
    "plt.title(col[1] + ' vs. ' + col[0] + ' with DBSCAN'); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(132)                          # plot the training data and K prototypes\n",
    "plt.scatter(df_in['n' + col[0]], df_in['n' + col[1]], c=df_in['DBSCAN'], alpha = 0.4, linewidths=1.0, edgecolors=\"black\", cmap = cmap)\n",
    "plt.scatter(df_outlier['n' + col[0]],df_outlier['n' +col[1]],c='black',s = 50,marker = 'x',edgecolor='k',cmap = cmap)\n",
    "# option to visualize the identified core samples\n",
    "#plt.scatter(dbscan.components_[:,0],dbscan.components_[:,1],c='red',s=50,marker = 's',edgecolor='k',cmap = cmap,alpha = 0.2)\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0] + ' Porosity with DBSCAN'); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' normalized')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(133)\n",
    "N, bins, patches  = plt.hist(df['DBSCAN'],alpha=0.2,edgecolor=\"black\",bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5,6.5],range=[0.5,2.5],density = True)\n",
    "patches[1].set_facecolor('black'); patches[3].set_facecolor('magenta'); patches[4].set_facecolor('red')\n",
    "patches[5].set_facecolor('yellow'); patches[6].set_facecolor('white'); patches[2].set_facecolor('gray');\n",
    "plt.title('Group Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.0, 6.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####  Spectral Clustering\n",
    "\n",
    "Spectral clustering is based on graph theory, where the sample data are mapped to a $n \\times n$ array that describes the degree of pairwise 'connectivity' between the sample data, and then Eigen vectors are calculated to reduce the dimensionality and k-means clustering is applied on the retained Eigen vectors to find the clusters.\n",
    "\n",
    "The advantages of the method includes:\n",
    "\n",
    "* the ability to encode connectivity between the sample dataset. While kernel and nearest neightbour approaches are commone, this step could include integration of various information sources.  \n",
    "\n",
    "* the Eigen values provide useful diagnostic information to inform the number of natural clusters based on the degree of cutting of connection required to separate the dataset\n",
    "\n",
    "* the Eigen vectors provide reduced dimensional representations of the high dimensional representation of pairwise connection\n",
    "\n",
    "**Other Resources** - I found the short article with demonstration on [Spectral Clustering](https://towardsdatascience.com/spectral-clustering-aba2640c0d5b) by William Fleshman quite useful to understand this approach. \n",
    "\n",
    "These are the steps for spectral clustering:\n",
    "\n",
    "* **Calculate the Simularity Graph and Matrix**\n",
    "\n",
    "* **Calculate the Degree Matrix**\n",
    "\n",
    "* **Calculate the Graph Laplacian Matrix**\n",
    "\n",
    "* **Perform Dimensionality Reduction on the Graph Laplacian Matrix**\n",
    "\n",
    "* **Assign Clusters in the Reduced Dimensionality**\n",
    "\n",
    "##### Calculate the Simularity Graph and Matrix\n",
    "\n",
    "Spectral clustering first calculates a graph of the data in predictor space\n",
    "\n",
    "* a graph is comprised of the samples (nodes) and connections (edges) between the samples\n",
    "\n",
    "There are mulitple methods to represent the sample data graph in predictor feature space:\n",
    "\n",
    "* **affinity matrix** the connections are represented as an $n \\times n$ matrix with 0 if no connection and a measure of similarity for connected samples. This is accomplished with radial basis function kernel with a specified gamma, or other kernels such as polynomial.\n",
    "\n",
    "* **adjacency matrix** the connections are represented as a $n \\times n$ matrix with 0 if no connection and 1 if connected.  This matrix may be calculated with k-nearest neighbor (up to a determined maximum number of neighbors), epsilon-neighborhood (all points withing a radius) or radial basis function kernel with a specified gamma, or other kernels such as polynomial. \n",
    "\n",
    "##### Calculate the Degree Matrix\n",
    "\n",
    "The degree matrix is a diagonal matrix where elements $i,i$ are the summation of the adjacency matrix row $i$.\n",
    "\n",
    "##### Calculate the Graph Laplacian Matrix \n",
    "\n",
    "The graph Laplacian is the adjacency matrix subtracted from the degree matrix.\n",
    "\n",
    "##### Perform Dimensionality Reduction on the Graph Laplacian Matrix\n",
    "\n",
    "Calculate the Eigen Values and Eigen Vectors of the Graph LaplacianThe result is $n$ Eigen vectors (of length $n$) and $n$ Eigen values.  We can interprete the Eigen values / vectors as follows:\n",
    "\n",
    "* the number of zero Eigen values are the number of connected parts of the dataset.  For example, when the simularity graph indicates 0 similarity for all sample data to sample data combinations, all Eigen values are 0, all sample data are independent connected parts. If all data are connected, then all Eigen values except for the first are nonzero, indicating all sample data in one connected part.\n",
    "\n",
    "* the first nonzero Eigen value is the **spectral gap**. It provides an indication of the density of the sample to sample connections. For example, if the affinity is 1.0 for all sample data pairs, the spectral gap is equal to $n$.\n",
    "\n",
    "* the second Eigen value is the **Fiedler value** and the associated Eigen vector is the **Fiedler vector**. It indicates the level of graph cut to separate the sample dataset into 2 parts. For example, if the second Eigen value is 0 already, then the dataset is already separated into atleast 2 parts; therefore, no cuts are needed!\n",
    "\n",
    "##### Assign Clusters in the Reduced Dimensionality\n",
    "\n",
    "The final step is to assign the clusters based on the result from the dimensionality reduced graph Laplacian matrix.  There are a couple of methods possible demonstrated here:\n",
    "\n",
    "1. We can use the Eigen values to determine the number of clusters:\n",
    "\n",
    "* jumps in Eigen values are used to identify the natural number of clusters in the dataset. i.e. if we add another connected group we would have to do a lot more cuts!\n",
    "\n",
    "2. Assignment of clusters by Eigen vector values:\n",
    "\n",
    "* the sign of the Fiedler vector provides the 2 custers (positive and negative) if the dataset where cut into 2 groups.\n",
    "\n",
    "3. Assignment of clusters by k-means clustering applied to the Eigen vectors \n",
    "\n",
    "* we apply k-means clustering on the Eigen vectors up to the determined number of clusters. This finds the groups asessed while integrating pairwise connections of the sample data.\n",
    "\n",
    "After some iteration I found this impementation satisfactory:\n",
    "\n",
    "* 5 clusters\n",
    "\n",
    "* affinity calculated with the rabial basis kernal applied to the sample data with a $\\gamma = 1000$\n",
    "\n",
    "* k-mean clustering applied to the $2^{nd}$ through $5^{th}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "spectral = SpectralClustering(n_clusters=K,assign_labels=\"kmeans\",affinity = 'rbf',\n",
    "                              gamma = 1000,n_neighbors = 200,random_state=230).fit(df[['n' + col[0],'n' + col[1]]].values)\n",
    "df['SPECTRAL'] = spectral.labels_ + 1\n",
    "\n",
    "cmap = plt.cm.inferno\n",
    "\n",
    "plt.subplot(131)                          # plot the assigned training data and K prototypes\n",
    "\n",
    "plt.scatter(df[col[0]], df[col[1]], c=df['SPECTRAL'], alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "plt.title(col[1] + ' vs. ' + col[0] + ' with Spectral Clustering'); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.subplot(132)                          # plot the training data and K prototypes\n",
    "plt.scatter(df['n' + col[0]], df['n' + col[1]], c=df['SPECTRAL'], alpha = 0.4, linewidths=1.0, edgecolors=\"black\", cmap = cmap)\n",
    "plt.title('Normalized ' + col[1] + ' vs. ' + col[0] + ' Spectral Clustering'); plt.xlabel(xlabel + ' (normalized)'); plt.ylabel(ylabel + ' (normalized)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(133)\n",
    "N, bins, patches  = plt.hist(df['SPECTRAL'],alpha=0.2,edgecolor=\"black\",bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5],range=[0.5,7.5],density = True)\n",
    "patches[1].set_facecolor('black'); patches[3].set_facecolor('red'); patches[4].set_facecolor('orange'); patches[5].set_facecolor('yellow') \n",
    "plt.title('Group Proportions')\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel(\"Proportion\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.0, 6.1, 1))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.2))\n",
    "ax.set_yticks(np.arange(0, 0.6, 0.05), minor=True)\n",
    "\n",
    "#plt.grid(which = 'both',color = 'black',alpha = 0.01)\n",
    "ax.grid(which='minor', alpha=0.1)\n",
    "ax.grid(which='major', alpha=0.2)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the affinity matrix that was calculated to drive this result.\n",
    "\n",
    "* this is the pairwise connection between all of the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111)                                          # plot the affinity matrix \n",
    "plt.imshow(spectral.affinity_matrix_,cmap = plt.cm.gist_heat_r,vmin = 0.0, vmax = 1.0)\n",
    "plt.xlabel(r'Sample Data $\\alpha = 1, \\ldots ,n$'); plt.ylabel(r'Sample Data $\\alpha = 1, \\ldots ,n$')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some general observations:\n",
    "\n",
    "* there is a large number of pairwise connections betwee the sample data\n",
    "\n",
    "* the data is ordered with a group from sample 0 to approximately sample 200, sample approximately 200 to sample approximately 1200 etc.  \n",
    "\n",
    "* the groups over 0-200 and 1200-1400 approximately appear to be close to each other\n",
    "\n",
    "Once again let's demonstrate the sensitivity of the solution to the model hyperparameters.\n",
    "\n",
    "Let's explore the sensivity in the parameters, by looking at the results over a $3 \\times 3$ combinatorial of hyperparameters. Run this the first time as is and then try making changes the the values in this code.\n",
    "\n",
    "```python\n",
    "ncluster_mat = [2,5,8] \n",
    "gamma_mat = [500,1000,2000]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncluster_mat = [2,3,4] \n",
    "gamma_mat = [500,1000,2000]\n",
    "\n",
    "index = 1\n",
    "for ncluster in ncluster_mat:\n",
    "    for gamma in gamma_mat:\n",
    "        print('Working on K = ' + str(ncluster) + ' and gamma ' + str(gamma))\n",
    "        spectral = SpectralClustering(n_clusters=ncluster,assign_labels=\"kmeans\",affinity = 'rbf',\n",
    "                              gamma = gamma,n_neighbors = 200,random_state=230).fit(df[['n' + col[0],'n' + col[1]]].values)\n",
    "        df['SPECTRAL'] = spectral.labels_ + 1\n",
    "        \n",
    "        plt.subplot(3,3,index)                          # plot the assigned training data and K prototypes\n",
    "        plt.scatter(df[col[0]], df[col[1]], c=df['SPECTRAL'], alpha=0.5, edgecolor='k', cmap = cmap)\n",
    "        plt.title(col[1] + ' vs. ' + col[0] + ' with Spectral, ncluster = ' + str(ncluster) + ', RBF: gamma = ' + str(gamma)); plt.xlabel('Porosity (%)'); plt.ylabel('Permeability (mD)')\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        index = index + 1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=3.50, wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations from these spectral cluster assignments:\n",
    "\n",
    "* as expected the method is able to devide the sample data into a specified number of clusters\n",
    "\n",
    "* as gamma increases, the pairwise affinity increases between the sample data, resulting in joining groups that now appear more connected\n",
    "\n",
    "#### Comments\n",
    "\n",
    "I hope you found this tutorial useful. I'm always happy to discuss data analytics, geostatistics, statistical modeling, uncertainty modeling and machine learning,\n",
    "\n",
    "I have other demonstrations on the basics of working with DataFrames, ndarrays, univariate statistics, plotting data, declustering, data transformations, trend modeling and many other workflows available at https://github.com/GeostatsGuy/PythonNumericalDemos and https://github.com/GeostatsGuy/GeostatsPy. \n",
    "  \n",
    "I hope this was helpful,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "#### The Author:\n",
    "\n",
    "### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "#### Want to Work Together?\n",
    "\n",
    "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
    "\n",
    "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
    "\n",
    "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
    "\n",
    "* I can be reached at mpyrcz@austin.utexas.edu.\n",
    "\n",
    "I'm always happy to discuss,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "I have other demonstrations on the basics of working with DataFrames, ndarrays, univariate statistics, plotting data, declustering, data transformations, trend modeling and many other workflows available at https://github.com/GeostatsGuy/PythonNumericalDemos and https://github.com/GeostatsGuy/GeostatsPy. \n",
    "  \n",
    "I hope this was helpful,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "#### More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
